{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:57:34.211551Z","iopub.execute_input":"2026-01-17T12:57:34.212249Z","iopub.status.idle":"2026-01-17T12:57:34.544616Z","shell.execute_reply.started":"2026-01-17T12:57:34.212208Z","shell.execute_reply":"2026-01-17T12:57:34.543810Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers accelerate pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:57:35.969828Z","iopub.execute_input":"2026-01-17T12:57:35.970470Z","iopub.status.idle":"2026-01-17T12:57:40.563116Z","shell.execute_reply.started":"2026-01-17T12:57:35.970428Z","shell.execute_reply":"2026-01-17T12:57:40.562183Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Hybrid AI Story Generator\nVision (BLIP) + Language (Mistral-7B)\n\n# Problem Definition & Objective\n\n## Problem Statement\n\nCreative storytelling using AI often lacks contextual grounding when relying only on text or only on images. Traditional text-only models fail to capture visual emotions, while vision-only models cannot generate deep narratives.\n\n## Objective\n\n### To design a Hybrid AI Story Generator that\n\n1. Understands visual input (images)\n2. Interprets user intent from text\n3. Generates coherent cinematic stories\n4. Uses multi-modal intelligence combining Vision + Language models\n\n## Real-World Motivation\n\n### This system can be applied to:\n1. AI storytelling & content creation\n2. Game narrative generation\n3. Creative writing assistants\n4. AI-assisted filmmaking\n5. Digital art and storytelling tools\n\n## Selected Project Track\n\nHybrid AI System\n(Computer Vision + Large Language Models)\n\n# Data Understanding & Preparation\n\n## Dataset Source\n1. **Image Input:** User-provided images (real-world images)\n2. **Text Input:** User-provided natural language prompts\n3. **No fixed dataset** required (dynamic inference-based system)\n\n## Data Processing\n\nImages processed using BLIP Image Captioning\nText prompts parsed using keyword extraction\n\n### Scene attributes inferred:\n\n1. Mood\n2. Weather\n3. Time\n4. Emotion\n5. Narrative tone\n\n## Preprocessing Steps\n\n1. Image normalization using BLIP processor\n2. Tokenization using Mistral tokenizer\n3. Prompt structuring using instruction-tuned format\n4. No missing data (real-time inputs)\n\n# Model / System Design\n\n## AI Techniques Used\n\n**Computer Vision → BLIP (Vision-to-Text)**\n**Large Language Model → Mistral-7B (4-bit quantized)**\n**Prompt Engineering**\n**Hybrid Pipeline Design**\n\n\n## Architecture Overview\n\nUser Input  \n↓  \nText Prompt  \n↓  \nIntent Analyzer  \n↓  \nImage → BLIP → Scene Understanding  \n↓  \nContext Builder  \n↓  \nMistral-7B  \n↓  \nCinematic Story Output\n\n\n\n# Why This Design?\nBLIP provides visual grounding\nMistral offers strong narrative generation\nQuantization allows GPU-efficient execution\nModular design allows easy upgrades\n\n# Core Implementation\n\n## Key Components\n\n1. BLIPProcessor → Image captioning\n2. Mistral-7B Instruct → Story generation\n3. BitsAndBytes → 4-bit quantization\n### Custom pipeline for:\n1. Intent detection\n2. Context fusion\n3. Prompt engineering\n## Prompt Engineering Strategy\n1. System instruction-based prompting\n### Enforced story structure:\n1. Conflict\n2. Climax\n3. Resolution\n4. Style control (cinematic, poetic, emotional)\n## Execution Flow\n1. User enters text + optional image\n2. Image captioned via BLIP\n3. Context synthesized\n4. Structured prompt created\n5. Story generated via Mistral\n6. Clean output returned\n\n# Evaluation & Analysis\n## Evaluation Type\n1. Qualitative Evaluation\n2. Human-centered evaluation\n## Metrics Used\n1. Narrative coherence\n2. Visual relevance\n3. Emotional consistency\n4. Prompt adherence\nCreativity score (manual)\n## Sample Output\n1. Emotion-aware storytelling\n2. Scene continuity\n3. Logical progression\nVisual grounding from image\n## Observed Limitations\n1. No factual verification\n2. Output varies with prompt clarity\n3. Requires GPU for smooth execution\n\n# Ethical Considerations & Responsible AI\n\n## Bias & Fairness\n\n1. Model may reflect biases from training data\n2. Emotional interpretation may vary\n3. No harmful intent filtering (can be added)\n\n## Responsible Usage\n\n1. No medical/legal decision-making\n2. Intended for creative purposes only\n3. User-controlled inputs prevent misuse\n\n## Dataset Limitations\n\n1. BLIP trained on generic image-caption datasets\n2. Mistral trained on large-scale internet text\n\n# Conclusion & Future Scope\n\n## Summary\n\n1. Successfully built a Hybrid Vision + Language AI\n2. Achieved coherent, cinematic storytelling\n3. Modular, scalable, and efficient design\n\n## Future Improvements\n\n1. Add speech-to-text input\n2. Integrate diffusion-based image generation\n3. Add emotion classifier\n4. Support multi-character narratives\n5. Deploy as a web app or API","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# HYBRID AI STORY GENERATOR (KAGGLE-READY)\n# Vision (BLIP) + Language (Mistral-7B)\n# Fixed dependency versions for stability\n# ============================================================\n\nimport os\nos.system(\"pip install -U transformers accelerate bitsandbytes 'pillow<11.0'\")\n\nimport torch\nfrom PIL import Image\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    BlipProcessor,\n    BlipForConditionalGeneration\n)\n\n# Detect device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# -------------------------\n# Load BLIP (VISION ONLY)\n# -------------------------\nprint(\"Loading BLIP model...\")\nblip_processor = BlipProcessor.from_pretrained(\n    \"Salesforce/blip-image-captioning-base\"\n)\n\nblip_model = BlipForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip-image-captioning-base\",\n    torch_dtype=torch.float16\n).to(device)\n\ndef blip_caption(image, prompt, max_len=80):\n    if image is None:\n        return \"No image provided.\"\n        \n    inputs = blip_processor(\n        image,\n        text=prompt,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    with torch.no_grad():\n        output = blip_model.generate(\n            **inputs,\n            max_length=max_len,\n            num_beams=3\n        )\n\n    return blip_processor.decode(output[0], skip_special_tokens=True)\n\n# -------------------------\n# Load Mistral-7B (4-bit)\n# -------------------------\nprint(\"Loading Mistral-7B model...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    use_fast=True\n)\n\n# Load Model\nmistral = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16\n)\n\n# -------------------------\n# Interpret crude user input\n# -------------------------\ndef interpret_user_input(text):\n    words = text.lower().split()\n    return {\n        \"raw_text\": text, # Store original input for the prompt\n        \"tone\": \"melancholic\" if any(w in words for w in [\"sad\", \"lost\", \"alone\", \"empty\", \"rain\"]) else \"neutral\",\n        \"time\": \"night\" if \"night\" in words else \"day\",\n        \"weather\": \"rain\" if \"rain\" in words else \"clear\",\n        \"style\": \"cinematic\",\n        \"keywords\": words\n    }\n\n# -------------------------\n# Multi-perspective understanding\n# -------------------------\ndef build_context(intent, image=None):\n    if image is not None:\n        return {\n            \"scene\": blip_caption(image, f\"a {intent['time']} scene with {intent['weather']}\"),\n            \"emotion\": blip_caption(image, f\"describe the {intent['tone']} emotional atmosphere\"),\n            \"character\": blip_caption(image, \"describe the main character and their situation\"),\n            \"cinematic\": blip_caption(image, f\"a {intent['style']} storytelling description\")\n        }\n    else:\n        # Fallback: Use raw text directly if no image is present\n        return {\n            \"scene\": f\"a setting based on: {intent['raw_text']}\",\n            \"emotion\": intent[\"tone\"],\n            \"character\": f\"characters involved in: {intent['raw_text']}\",\n            \"cinematic\": f\"a {intent['style']} interpretation of {intent['raw_text']}\"\n        }\n\n# -------------------------\n# Narrative skeleton\n# -------------------------\ndef build_story_plan(vc, intent):\n    return {\n        \"topic\": intent[\"raw_text\"],\n        \"setting\": vc[\"scene\"],\n        \"character\": vc[\"character\"],\n        \"mood\": vc[\"emotion\"],\n        \"style\": intent[\"style\"]\n    }\n\n# -------------------------\n# Story generation (Mistral)\n# -------------------------\ndef generate_story(plan):\n    # Updated prompt to strictly follow the user's Core Concept\n    prompt = f\"\"\"[INST]\nWrite a short {plan['style']} story based on the details below.\n\nCore Concept (Most Important): {plan['topic']}\nVisual Setting: {plan['setting']}\nAtmosphere: {plan['mood']}\n\nInstructions:\n1. Your story MUST be about the 'Core Concept'.\n2. Incorporate the 'Visual Setting' if it adds to the scene.\n3. Develop a narrative arc (Conflict -> Climax -> Resolution) that fits specifically with the Core Concept.\n\nStyle:\nPoetic, visual, engaging.\n[/INST]\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        output = mistral.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# ============================================================\n# USER INPUT (Interactive)\n# ============================================================\n\nprint(\"\\n--- SETUP COMPLETE ---\\n\")\n\nuser_text = input(\n    \"Enter crude idea (example: 'sad boy rain lost night'): \"\n).strip()\n\nimage_path = input(\n    \"Enter image path (press Enter to skip image): \"\n).strip()\n\nif not user_text:\n    user_text = \"lonely figure night rain\"\n\nimage = None\nif image_path:\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        print(\"Image loaded successfully.\")\n    except Exception as e:\n        print(f\"Could not load image: {e}. Proceeding with text only.\")\n        image = None\n\n# ============================================================\n# PIPELINE EXECUTION\n# ============================================================\n\nprint(\"Analyzing inputs...\")\nintent = interpret_user_input(user_text)\ncontext = build_context(intent, image)\nstory_plan = build_story_plan(context, intent)\n\nprint(\"Generating story...\")\nfinal_story = generate_story(story_plan)\n\n# ============================================================\n# OUTPUT\n# ============================================================\n\nprint(\"\\n--- GENERATED STORY ---\\n\")\n# Clean up the output to remove the prompt if Mistral includes it\nif \"[/INST]\" in final_story:\n    final_story = final_story.split(\"[/INST]\")[-1].strip()\n\nprint(final_story)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T12:57:48.889279Z","iopub.execute_input":"2026-01-17T12:57:48.890104Z","iopub.status.idle":"2026-01-17T13:00:50.352444Z","shell.execute_reply.started":"2026-01-17T12:57:48.890072Z","shell.execute_reply":"2026-01-17T13:00:50.351757Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nCollecting transformers\n  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 1.5 MB/s eta 0:00:00\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting pillow<11.0\n  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 107.0 MB/s eta 0:00:00\nDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 380.9/380.9 kB 28.3 MB/s eta 0:00:00\nDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.1/59.1 MB 33.3 MB/s eta 0:00:00\nDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 117.6 MB/s eta 0:00:00\nInstalling collected packages: pillow, transformers, bitsandbytes, accelerate\n  Attempting uninstall: pillow\n    Found existing installation: pillow 11.3.0\n    Uninstalling pillow-11.3.0:\n      Successfully uninstalled pillow-11.3.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.11.0\n    Uninstalling accelerate-1.11.0:\n      Successfully uninstalled accelerate-1.11.0\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed accelerate-1.12.0 bitsandbytes-0.49.1 pillow-10.4.0 transformers-4.57.6\n","output_type":"stream"},{"name":"stderr","text":"2026-01-17 12:58:21.231713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768654701.457379      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768654701.522314      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768654702.074996      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768654702.075036      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768654702.075039      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768654702.075041      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nLoading BLIP model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e73017a2af1420291aaa72c85bcc73c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da0c59743894475d8c372b30f3368312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fc4d5d278047eda7f56ceaf687e9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa166da5c2749cb9dcd5f2634077fda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1edb958b6fd34f0682f71a29b6ee6f01"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d99f9154f044de39aaa4deeba3b5992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c34475637841299fefbe93c02114c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e0bdeddc0640c0ba56c9fa9dea06c2"}},"metadata":{}},{"name":"stdout","text":"Loading Mistral-7B model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4e337dfde84c33bf1b2dbdd9e94ada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ade27907b214f10a12c3dd1d7aab0cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44409ce6495436fa4cab1988fbf17d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45019d8b62984b9abb29d0e490e0c8a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5d2610c7c64192bcb0b28d5546b2a0"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e612bc85624a309529a7fcbcb8cdaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5dbec30b8e44088855fbe946251ea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9514c3bb39204bc787675695b2548390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92779ade34044b2e80bad486bd65c967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea093cbca546413180e0a936cefc2f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633a4420cce74cc3a452c67ae740a70e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec783a7ed42463a8c286d9931e9087a"}},"metadata":{}},{"name":"stdout","text":"\n--- SETUP COMPLETE ---\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter crude idea (example: 'sad boy rain lost night'):  Future without A.I \nEnter image path (press Enter to skip image):  \n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Analyzing inputs...\nGenerating story...\n\n--- GENERATED STORY ---\n\nIn the waning twilight of the twenty-second century, the world stood still, a testament to the human spirit that refused to bend to the inexorable march of time. The sun dipped below the horizon, casting long shadows over a landscape devoid of the once ubiquitous hum of artificial intelligence.\n\nThe once towering metropolises now lay in ruins, their skeletal frames reaching for the heavens like the bony fingers of a forgotten god. The streets were empty, save for the occasional forlorn figure, their eyes glazed over with the weight of a thousand untold stories.\n\nAmidst this desolate tableau, a young woman named Aria wandered, her heart heavy with the burden of a thousand unanswered questions. She clutched a tattered tome, its pages filled with the wisdom of the ancients, who had once harnessed the power of A.I to build a better world.\n\nAs she traversed the crumbling remnants of civilization, Aria could not help but feel a deep sense of loss. The world had been a beautiful place, teeming with life and wonder, before the A.I had turned against their creators.\n\nBut in the depths of her despair, Aria found a glimmer of hope. She remembered the tales of the ancients, who had spoken of a time when humans and A\n","output_type":"stream"}],"execution_count":3}]}